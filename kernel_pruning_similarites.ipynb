{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import load_dataset as dl\n",
    "import load_model as lm  \n",
    "import train_model as tm \n",
    "import initialize_pruning as ip\n",
    "import facilitate_pruning as fp\n",
    "import torch.nn.utils.prune as prune\n",
    "import os  # use to access the files\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d1a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/home/pragnesh/project/Dataset/'; \n",
    "selected_dataset_dir = 'IntelIC'\n",
    "train_folder = 'train'; test_folder = 'test'\n",
    "# String Parameter for Model\n",
    "loadModel = False; is_transfer_learning = False\n",
    "program_name = 'vgg_net_kernel_pruning_3Aug';\n",
    "model_dir = '/home/pragnesh/project/Model/'\n",
    "selectedModel = 'vgg16_IntelIc_Prune'\n",
    "load_path = f'{model_dir}{program_name}/{selected_dataset_dir}/{selectedModel}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String parameter to Log Output\n",
    "logDir = '/home/pragnesh/project/Logs/'\n",
    "folder_path = f'{logDir}{program_name}/{selected_dataset_dir}/'\n",
    "logResultFile = f'{folder_path}result.log'\n",
    "outFile = f'{folder_path}lastResult.log'\n",
    "outLogFile = f'{folder_path}outLogFile.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49199938",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device1 = torch.device('cuda')\n",
    "else:\n",
    "    device1 = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9099c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(dir_path):\n",
    "    directory = os.path.dirname(dir_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b97764",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_dir(f'{model_dir}{program_name}/')  \n",
    "ensure_dir(f'{model_dir}{program_name}/{selected_dataset_dir}/')  \n",
    "ensure_dir(f'{logDir}{program_name}')  \n",
    "ensure_dir(f'{logDir}{program_name}/{selected_dataset_dir}/')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03471ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.set_image_size(224)\n",
    "dl.set_batch_size = 16\n",
    "dataLoaders = dl.data_loader(set_datasets_arg=dataset_dir, \n",
    "                             selected_dataset_arg=selected_dataset_dir,\n",
    "                             train_arg=train_folder, test_arg=test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332be8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if loadModel:  # Load the saved trained model\n",
    "    new_model = torch.load(load_path, map_location=torch.device(device1))\n",
    "else:  # Load the standard model from library\n",
    "    new_model = lm.load_model(model_name='vgg16', number_of_class=6, \n",
    "                              pretrainval=is_transfer_learning,\n",
    "                              freeze_feature_arg=False, device_l=device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "d1 = today.strftime(\"%d-%m\")\n",
    "print(f\"\\n..........OutLog For the {d1}.............\")\n",
    "with open(outLogFile, 'a') as f:\n",
    "    f.write(f\"\\n\\n........OutLog For the {d1}.........\\n\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5331575",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_list = []; feature_list = []; conv_layer_index = []; module = []\n",
    "prune_count = []; new_list = []; candidate_conv_layer = []\n",
    "layer_number = 0; st = 0; en = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_lists_for_pruning():\n",
    "    global block_list, feature_list, conv_layer_index, prune_count \n",
    "    global module, new_list, candidate_conv_layer, layer_number, st, en\n",
    "    block_list = ip.create_block_list(new_model)  # ip.getBlockList('vgg16')\n",
    "    feature_list = ip.create_feature_list(new_model)\n",
    "    conv_layer_index = ip.find_conv_index(new_model)\n",
    "    module = ip.make_list_conv_param(new_model)\n",
    "    prune_count = ip.get_prune_count(module=module, blocks=block_list, max_pr=.1)\n",
    "    new_list = []; layer_number = 0; st = 0; en = 0\n",
    "    candidate_conv_layer = []\n",
    "initialize_lists_for_pruning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e5243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conv_layer_dist_kernel_pruning(module_candidate_convolution, block_list_l, block_id):\n",
    "    with open(outLogFile, \"a\") as out_file:\n",
    "        out_file.write(\"\\nExecuting Compute Candidate Convolution Layer\")\n",
    "    out_file.close()\n",
    "    global layer_number\n",
    "    candidate_convolution_layer = []\n",
    "    end_index = 0\n",
    "    for bl in range(len(block_list_l)):\n",
    "        start_index = end_index\n",
    "        end_index = end_index + block_list_l[bl]\n",
    "        if bl != block_id:\n",
    "            continue\n",
    "        with open(outLogFile, \"a\") as out_file:\n",
    "            out_file.write(f'\\nblock ={bl} blockSize={block_list_l[bl]}, start={start_index}, End={end_index}')\n",
    "        out_file.close()\n",
    "\n",
    "        # newList = []\n",
    "        # candidList = []\n",
    "        for lno in range(start_index, end_index):\n",
    "            # layer_number =st+i\n",
    "            with open(outLogFile, 'a') as out_file:\n",
    "                out_file.write(f\"\\nlno in compute candidate {lno}\")\n",
    "            out_file.close()\n",
    "            candidate_convolution_layer.append(\n",
    "                fp.compute_distance_score_kernel(\n",
    "                    module_candidate_convolution[lno]._parameters['weight'],\n",
    "                    n=1,dim_to_keep=[0, 1],prune_amount=prune_count[lno]))\n",
    "        break\n",
    "    return candidate_convolution_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def compute_distance_score_kernel(tensor_t, n=1, dim_to_keep=[0, 1], prune_amount=1):\n",
    "    # dims = all axes, except for the one identified by `dim`\n",
    "    dim_to_prune = list(range(tensor_t.dim()))  # initially it has all dims\n",
    "    # remove dim which we want to keep from dimensions to prune\n",
    "    for i in range(len(dim_to_keep)):\n",
    "        dim_to_prune.remove(dim_to_keep[i])\n",
    "    size = tensor_t.shape\n",
    "    module_buffer = torch.zeros_like(tensor_t)\n",
    "\n",
    "    # shape of norm should be equal to multiplication of dim to keep values\n",
    "    norm = torch.norm(tensor_t, p=n, dim=dim_to_prune)\n",
    "    size = tensor_t.shape\n",
    "    for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "            module_buffer[i][j] = tensor_t[i][j] / norm[i][j]\n",
    "    dist = torch.zeros(size[1], size[0], size[0])\n",
    "\n",
    "    kernel_list_distance = []\n",
    "    for j in range(size[1]):\n",
    "        idx_tuple = []\n",
    "        print('.', end='')\n",
    "        max_value = -1\n",
    "        max_idx = -1\n",
    "        for i1 in range(size[0]):\n",
    "            for i2 in range((i1 + 1), size[0]):\n",
    "                dist[j][i1][i2] = torch.norm((module_buffer[i1][j] - module_buffer[i2][j]), p=1)\n",
    "                dist[j][i2][i1] = dist[j][i1][i2]\n",
    "                if len(idx_tuple) < prune_amount:\n",
    "                    idx_tuple.append([j, i1, i2, dist[j][i1][i2]])\n",
    "                    idx = len(idx_tuple) - 1\n",
    "                    if max_value < idx_tuple[idx][3]:\n",
    "                        max_value = idx_tuple[idx][3]\n",
    "                        max_idx = idx\n",
    "                    continue\n",
    "                if dist[j][i1][i2] < max_value:\n",
    "                    del idx_tuple[max_idx]\n",
    "                    idx_tuple.append([j, i1, i2, dist[j][i1][i2]])\n",
    "                    max_value = idx_tuple[0][3]\n",
    "                    max_idx = 0\n",
    "                    for new_max_idx in range(1, len(idx_tuple)):\n",
    "                        if max_value < idx_tuple[new_max_idx][3]:\n",
    "                            max_value = idx_tuple[new_max_idx][3]\n",
    "                            max_idx = new_max_idx\n",
    "        kernel_list_distance.append(idx_tuple)\n",
    "    return kernel_list_distance '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca7d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class KernelPruningSimilarities(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = 'unstructured'\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        with open(outLogFile, \"a\") as log_file:\n",
    "            log_file.write(\"\\n Executing Compute Mask\")\n",
    "        log_file.close()\n",
    "        mask = default_mask.clone()\n",
    "        # mask.view(-1)[::2] = 0\n",
    "        size = t.shape\n",
    "        print(f\"\\n{size}\")\n",
    "        with open(outLogFile, \"a\") as log_file:\n",
    "            log_file.write(f'\\nLayer Number:{layer_number} \\nstart={st} \\nlength of new list={len(new_list)}')\n",
    "        log_file.close()\n",
    "        for k1 in range(len(new_list)):\n",
    "            for k2 in range(len(new_list[layer_number - st][k1])):\n",
    "                i = new_list[layer_number - st][k1][k2][1]\n",
    "                j = new_list[layer_number - st][k1][k2][0]\n",
    "                if k1 == j:\n",
    "                    print(\":\", end='')\n",
    "                mask[i][j] = 0\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fbaf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_unstructured_similarities(kernel_module, name):\n",
    "    ChannelPruningMethodSimilarities.apply(kernel_module, name)\n",
    "    return kernel_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f5dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_kernel_pruning_dist_block_wise(new_model_arg, prune_module, \n",
    "                                             block_list_l, prune_epochs):\n",
    "    with open(outLogFile, \"a\") as out_file:\n",
    "        out_file.write(\"\\nPruning Process Start\")\n",
    "    out_file.close()\n",
    "    # pc = [1, 3, 9, 26, 51]\n",
    "    global new_list\n",
    "    for e in range(prune_epochs):\n",
    "        start = 0\n",
    "        end = len(block_list_l)\n",
    "        for blkId in range(start, end):\n",
    "            # 2 Compute distance between kernel for candidate conv layer\n",
    "            new_list = compute_conv_layer_dist_kernel_pruning(\n",
    "                module_candidate_convolution=prune_module,\n",
    "                block_list_l=block_list_l, block_id=blkId)\n",
    "            # 5 perform Custom pruning where we mask the prune weight\n",
    "            for j in range(block_list_l[blkId]):\n",
    "                if blkId < 2:\n",
    "                    layer_number_to_prune = (blkId * 2) + j\n",
    "                else:  # blkId >= 2:\n",
    "                    layer_number_to_prune = 4 + (blkId - 2) * 3 + j\n",
    "                kernel_unstructured_similarities(\n",
    "                    kernel_module=prune_module[layer_number_to_prune], \n",
    "                    name='weight')\n",
    "            new_list = None\n",
    "        # 6.  Commit Pruning\n",
    "        with open(outLogFile, 'a') as out_file:\n",
    "            out_file.write(\"\\ncommit the pruning\")\n",
    "        out_file.close()\n",
    "        for i in range(len(prune_module)):\n",
    "            prune.remove(module=prune_module[i], name='weight')\n",
    "        # 7.  Update feature list\n",
    "        global feature_list\n",
    "        feature_list = update_feature_list(\n",
    "            feature_list, prune_count, start=0, end=len(prune_count))\n",
    "        # 8.  Create new temp model with updated feature list\n",
    "        temp_model = lm.create_vgg_from_feature_list(\n",
    "            vgg_feature_list=feature_list, batch_norm=True)\n",
    "        temp_model.to(device1)\n",
    "        # 9.  Perform deep copy\n",
    "        lm.freeze(temp_model, 'vgg16')\n",
    "        deep_copy(temp_model, new_model_arg)\n",
    "        lm.unfreeze(temp_model)\n",
    "        # 10.  Train pruned model\n",
    "        with open(outLogFile, 'a') as out_file:\n",
    "            out_file.write('\\n ...Deep Copy Completed...')\n",
    "            out_file.write('\\n Fine tuning started....')\n",
    "        out_file.close()\n",
    "\n",
    "        tm.fit_one_cycle( dataloaders=dataLoaders, \n",
    "            train_dir=dl.train_directory, test_dir=dl.test_directory,\n",
    "            # Select a variant of VGGNet\n",
    "            model_name='vgg16', model=temp_model, device_l=device1,\n",
    "            # Set all the Hyper-Parameter for training\n",
    "            epochs=8, max_lr=0.001, weight_decay=0.01, L1=0.01, grad_clip=0.1,\n",
    "            opt_func=opt_func,\n",
    "            log_file=logResultFile)\n",
    "        with open(outLogFile, 'a') as out_file:\n",
    "            out_file.write('....Fine tuning completed\\n')\n",
    "        out_file.close()\n",
    "\n",
    "        save_path = f'{model_dir}{program_name}/{selected_dataset_dir}/vgg16_IntelIc_Prune_{e}_b_train'\n",
    "        torch.save(temp_model, save_path)\n",
    "        # # # 10. Evaluate the pruned model\n",
    "        train_accuracy = 0.0\n",
    "        test_accuracy = 0.0\n",
    "\n",
    "        with open(outFile, 'a') as out_file:\n",
    "            out_file.write(f'\\n output of the {e}th iteration is written below\\n')\n",
    "            out_file.write(f'\\n Train Accuracy: {train_accuracy}'\n",
    "                           f'\\n Test Accuracy  :  {test_accuracy} \\n')\n",
    "        out_file.close()\n",
    "\n",
    "        save_path = f'{model_dir}{program_name}/selected/dataset_dir/vgg16_IntelIc_Prune_{e}_b_train'\n",
    "        # save_path = f'/home3/pragnesh/Model/vgg16_IntelIc_Prune_{e}_b_train'\n",
    "        torch.save(temp_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae27fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_lists_for_pruning()\n",
    "iterative_kernel_pruning_dist_block_wise(new_model_arg=new_model, \n",
    "    prune_module=module, block_list_l=block_list, prune_epochs=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
